{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "vhdvJkZ4CsNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TurDgCxDuUo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# check whether run in Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install timm==0.5.4 "
      ],
      "metadata": {
        "id": "6Uu52V6UuR88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import csv\n",
        "import cv2 # for image load\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import albumentations as A\n",
        "import albumentations.pytorch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import timm\n",
        "\n",
        "import random\n",
        "import plotly.express as px # for grap\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "\n",
        "from sklearn.metrics import f1_score # for f1 score\n",
        "\n",
        "from tqdm import tqdm # for progress bar"
      ],
      "metadata": {
        "id": "zTdH3M8ACw22"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# avail_pretrained_models = timm.list_models(pretrained=True)\n",
        "# len(avail_pretrained_models), avail_pretrained_models"
      ],
      "metadata": {
        "id": "DUJ9rEPmCtwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Dataset"
      ],
      "metadata": {
        "id": "TmQRdAeG7hMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MY_DIR: 자기 이미지 압축파일 있는 경로 넣기\n",
        "# MY_DIR = '/content/drive/MyDrive/ML/DATA302/dataset'\n",
        "MY_DIR = '/content/drive/MyDrive/3d'\n",
        "\n",
        "FAKE_IMG_PATH = '/content/TestSet/'\n",
        "REAL_IMG_PATH = '/content/REAL/'"
      ],
      "metadata": {
        "id": "5vHRX7vDwzTY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(f\"unzip {MY_DIR}/TestSet.zip -d /content/\")\n",
        "!mkdir \"tars\"\n",
        "os.system(f\"tar -xvf {MY_DIR}/ILSVRC2012_img_train_t3.tar -C /content/tars\")\n",
        "\n",
        "!mkdir \"REAL\"\n",
        "dir_list = os.listdir(\"/content/tars\")\n",
        "for dir in dir_list:\n",
        "  os.system(f\"tar -xvf /content/tars/{dir} -C /content/REAL\")"
      ],
      "metadata": {
        "id": "rdF8WmbB6kLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REAL 디렉토리 안에 아무것도 안뜨면 이 코드 한번 돌려보셈\n",
        "# os.listdir(\"/content/REAL\")"
      ],
      "metadata": {
        "id": "GQcawFQl1CAq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Env"
      ],
      "metadata": {
        "id": "f6fH5F4QC6oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "model_list = {\n",
        "    \"convnext_base_in22ft1k\" : {\n",
        "        \"model\" : \"convnext_base_in22ft1k\", \n",
        "        \"input_size\" : (224, 224),\n",
        "        \"classifier_in_feature\" : 1024\n",
        "        }\n",
        "}\n",
        "\n",
        "optimizer_list = [\"adam\", \"sgd\"]\n",
        "\n",
        "model_name = \"convnext_base_in22ft1k\"\n",
        "\n",
        "CONFIG = {\n",
        "    \"batch_size\" : 16,\n",
        "    \"epoch\" : 200,\n",
        "    \"learning_rate\" : 1e-4,\n",
        "    \"input_size\" : model_list[model_name][\"input_size\"],\n",
        "    \"backbone\" : model_list[model_name][\"model\"],\n",
        "    \"classifier_in_feature\": model_list[model_name][\"classifier_in_feature\"],\n",
        "    \"device\" : device,\n",
        "    \"patience\" : 10,\n",
        "    \"optimizer\" : optimizer_list[0],\n",
        "    \"input_norm_mean\" : IMAGENET_DEFAULT_MEAN,\n",
        "    \"input_norm_std\" : IMAGENET_DEFAULT_STD\n",
        "}\n",
        "\n",
        "batch_size = CONFIG[\"batch_size\"]\n",
        "epochs = CONFIG[\"epoch\"]\n",
        "learning_rate = CONFIG[\"learning_rate\"]\n",
        "input_size = CONFIG[\"input_size\"]\n",
        "backbone = CONFIG[\"backbone\"]\n",
        "classifier_in_feature = CONFIG[\"classifier_in_feature\"]\n",
        "device = CONFIG[\"device\"]\n",
        "patience = CONFIG[\"patience\"]\n",
        "optimizer = CONFIG[\"optimizer\"]\n",
        "input_norm_mean = CONFIG[\"input_norm_mean\"]\n",
        "input_norm_std = CONFIG[\"input_norm_std\"]"
      ],
      "metadata": {
        "id": "4IZGDkseC8M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the Data"
      ],
      "metadata": {
        "id": "4DKFSj1mDxcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, FAKE_path_label, REAL_path_label, input_size, transform=None, isTest=False):\n",
        "\n",
        "        self.FAKE_path_label = FAKE_path_label\n",
        "        self.REAL_path_label = REAL_path_label\n",
        "        self.input_size = input_size\n",
        "        self.transform = transform\n",
        "        self.isTest = isTest\n",
        "        self.img_list = []\n",
        "\n",
        "        for i in range(len(FAKE_path_label)):\n",
        "            img_path = FAKE_path_label[i][0]\n",
        "            image = Image.open(img_path)\n",
        "            if self.transform:\n",
        "                image = self.transform(image=np.array(image))['image']\n",
        "            self.img_list.append((image, 0))\n",
        "\n",
        "        for i in range(len(REAL_path_label)):\n",
        "            img_path = REAL_path_label[i][0]\n",
        "            image = Image.open(img_path)\n",
        "            if self.transform:\n",
        "                image = self.transform(image=np.array(image))['image']\n",
        "            self.img_list.append((image, 1))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.img_list[idx]\n",
        "\n",
        "        return image, label #label type: int"
      ],
      "metadata": {
        "id": "l25G2NX3D0ka"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FAKE_path_label = []\n",
        "REAL_path_label = []\n",
        "\n",
        "# Get Fake Data\n",
        "with open(os.path.join(FAKE_IMG_PATH, \"operations.csv\"), \"r\") as operations_csv:\n",
        "    row = csv.reader(operations_csv, delimiter=',')\n",
        "    for i, col in enumerate(row):\n",
        "        if col[6] == \"FALSE\" and int(col[1]) >= 224: # col[6]: label, col[1]: cropsize\n",
        "            FAKE_path_label.append((FAKE_IMG_PATH + col[0], 0)) # col[0]: src, 0 for false\n",
        "\n",
        "\n",
        "# Get Real Data\n",
        "REAL_path_label = [(os.path.join(REAL_IMG_PATH, path), 1) for path in os.listdir(REAL_IMG_PATH)]\n",
        "\n",
        "A_transform = A.Compose([\n",
        "    A.PadIfNeeded(*input_size),\n",
        "    albumentations.augmentations.crops.transforms.CenterCrop(*input_size),\n",
        "    albumentations.augmentations.transforms.Normalize(\n",
        "        mean=input_norm_mean, \n",
        "        std=input_norm_std\n",
        "        ),\n",
        "    albumentations.pytorch.transforms.ToTensorV2()\n",
        "])\n",
        "\n",
        "FAKE_path_label = random.sample(FAKE_path_label, k=3000) # 2000 for training, 1000 for test\n",
        "REAL_path_label = random.sample(REAL_path_label, k=3000) # 2000 for training, 1000 for test\n",
        "\n",
        "labeled_dataset = CustomDataset(FAKE_path_label[:2000], REAL_path_label[:2000], input_size=input_size, transform=A_transform, isTest=False)\n",
        "# train_dataloader = DataLoader(\n",
        "#     dataset=labeled_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "\n",
        "test_dataset = CustomDataset(FAKE_path_label[2000:], REAL_path_label[2000:], input_size=input_size, transform=A_transform, isTest=True)\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "def dataset_split(dataset, ratio):\n",
        "    train_size = int(ratio * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "Jw0SDxzUEAPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping"
      ],
      "metadata": {
        "id": "1RD_eJADSIc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.best_score = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}.\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "c28vVsrvSIJ6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "cb6b2LYdFWrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, backbone, classifier_in_feature, pretrained=True):\n",
        "        super(Model, self).__init__()\n",
        "        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n",
        "        self.backbone.reset_classifier(0)\n",
        "        self.classifier = nn.Linear(\n",
        "                in_features=classifier_in_feature,\n",
        "                out_features=2\n",
        "            )\n",
        "        \n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad=False\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.backbone(input) # bs 1 1000\n",
        "        output = self.classifier(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Model(backbone, classifier_in_feature, pretrained=True)\n",
        "model = model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "if optimizer == \"adam\":\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
        "elif optimizer == \"sgd\":\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "BmPfmZSwFWSN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model)"
      ],
      "metadata": {
        "id": "23rlubhn-2Mf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Validation Function"
      ],
      "metadata": {
        "id": "r8YCKFBSUBdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(device, model, optimizer, loss_fn, dataloader):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    running_loss = 0\n",
        "    pred_ans, gt_ans = [], []\n",
        "\n",
        "    for batch, (x, gt) in enumerate(tqdm(dataloader)):\n",
        "        x, gt = x.to(device), gt.to(device) \n",
        "        output = model(x)\n",
        "        loss = loss_fn(output, gt)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss\n",
        "        \n",
        "        pred = output.argmax(dim=1)\n",
        "        pred_ans += pred.tolist()\n",
        "        gt_ans += gt.tolist()\n",
        "        correct += torch.sum(pred == gt.data)\n",
        "\n",
        "    acc = 100 * correct / (len(dataloader) * batch_size)\n",
        "    running_loss = running_loss/len(dataloader)\n",
        "\n",
        "    return running_loss, acc, pred_ans, gt_ans\n",
        "\n",
        "@torch.no_grad()\n",
        "def valid_model(device, model, dataloader):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    pred_ans, gt_ans = [], []\n",
        "\n",
        "    for batch, (x, gt) in enumerate(dataloader):\n",
        "        x, gt = x.to(device), gt.to(device)\n",
        "\n",
        "        output = model(x)\n",
        "        loss = loss_fn(output, gt).item()\n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "\n",
        "        running_loss += loss\n",
        "        pred_ans += pred.tolist()\n",
        "        gt_ans += gt.tolist()\n",
        "        correct += torch.sum(pred == gt)\n",
        "        \n",
        "    acc = 100 * correct / (len(dataloader) * batch_size)\n",
        "    running_loss = running_loss/len(dataloader)\n",
        "\n",
        "    return running_loss, acc, pred_ans, gt_ans\n"
      ],
      "metadata": {
        "id": "SWBSkfPPUBTP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Nz4zXAOWWn57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run = wandb.init(\n",
        "#             project=\"3d\",\n",
        "#             config=CONFIG\n",
        "#             ) # start a new run"
      ],
      "metadata": {
        "id": "b-NolrwEYk_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"best_model.pt\"\n",
        "val_loss_list = []\n",
        "train_loss_list = []\n",
        "cur_loss_min = 100000000\n",
        "\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=False)\n",
        "\n",
        "for e in range(epochs):\n",
        "    print(f\"\\nEpoch {e+1}\")\n",
        "    train_dataset, val_dataset = dataset_split(labeled_dataset, 0.8) # 8 : 2 = train : val\n",
        "    train_dataloader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "    val_dataloader = DataLoader(\n",
        "        dataset=val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    train_loss, train_acc, train_pred, train_gt = train_model(device, model, optimizer, loss_fn, train_dataloader)\n",
        "    val_loss, val_acc, val_pred, val_gt = valid_model(device, model, val_dataloader)\n",
        "\n",
        "    # wandb.log({\"train loss\": train_loss, \n",
        "    #            \"train accuracy\": train_acc, \n",
        "    #            \"validation loss\": val_loss, \n",
        "    #            \"validation accuracy\": val_acc})\n",
        "\n",
        "    early_stopping(val_loss, model)\n",
        "    print(f\"train loss: {train_loss}\\tvalidation loss: {val_loss}\")\n",
        "\n",
        "    if val_loss < cur_loss_min:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(f\"Validation loss reduced ({cur_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
        "        cur_loss_min = val_loss\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "    val_loss_list.append(val_loss)\n",
        "    train_loss_list.append(train_loss)\n"
      ],
      "metadata": {
        "id": "AoPs1kkrWsqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "ym8sC7iaMhR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test_model(device, model, dataloader):\n",
        "    pred_ans = []\n",
        "    gt_ans = []\n",
        "    corrects = 0\n",
        "\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    for x, gt in tqdm(dataloader):  # tqdm: progress bar 표시\n",
        "        x, gt = x.to(device), gt.to(device)\n",
        "        output = model(x)\n",
        "        \n",
        "        pred = output.argmax(dim=1)\n",
        "        \n",
        "        pred_ans += pred.tolist()\n",
        "        gt_ans += gt.tolist()\n",
        "        corrects += torch.sum(pred == gt.data)\n",
        "\n",
        "    # accuracy 출력\n",
        "    print(f\"total accuracy: {corrects / len(dataloader.dataset)}\")\n",
        "\n",
        "    return pred_ans, gt_ans\n"
      ],
      "metadata": {
        "id": "YQKyunHXMiwd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools    # confusion matrix에서 사용\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(label, pred, target_names=None, labels=True):\n",
        "    cm = confusion_matrix(label, pred)\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    thresh = cm.max() / 2\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if labels:\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]), horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i,j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Nh2Y5WVTTd0r"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_ans, gt_ans = test_model(device, model, test_dataloader)\n",
        "plot_confusion_matrix(pred_ans, gt_ans)"
      ],
      "metadata": {
        "id": "sNzboU3LU8HU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jham6ZH4U_Ba"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}